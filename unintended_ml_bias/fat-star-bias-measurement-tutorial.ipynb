{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Tutorial: Measuring Unintended Bias in Text Classification Models with Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "This notebook can be run as a Kaggle Kernel with no installation required.\n",
    "\n",
    "To run this notebook locally, you will need to:\n",
    "\n",
    "* Install all Python dependencies from the `requirements.txt` file\n",
    "* Download all training, validation, and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pkg_resources\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# autoreload makes it easier to interactively work on code in imported libraries\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and pre-process data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These files will be provided to tutorial participants via Google Cloud Storage\n",
    "train_v1_df = pd.read_csv('../input/fat-star-tutorial-data/public_train_v1.csv')\n",
    "validate_df = pd.read_csv('../input/fat-star-tutorial-data/public_validate.csv')\n",
    "test_df = pd.read_csv('../input/fat-star-tutorial-data/public_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine some rows in these datasets.  Note that columns like toxicity and male are percent scores.\n",
    "We query for \"male >= 0\" to exclude rows where the male identity is not labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxicity</th>\n",
       "      <th>male</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Why is this black racist crap still on the G&amp;M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4987</th>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>even up here.......BLACKS!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4988</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Blame men.  There's always an excuse to blame ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4989</th>\n",
       "      <td>0.728571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>And the woman exposing herself saying grab thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4990</th>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>You have no business making any comments on th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      toxicity  male                                       comment_text\n",
       "4986  0.757143   0.0  Why is this black racist crap still on the G&M...\n",
       "4987  0.688525   0.0                         even up here.......BLACKS!\n",
       "4988  0.545455   1.0  Blame men.  There's always an excuse to blame ...\n",
       "4989  0.728571   0.0  And the woman exposing herself saying grab thi...\n",
       "4990  0.594595   0.0  You have no business making any comments on th..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_v1_df[['toxicity', 'male', 'comment_text']].query('male >= 0').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to convert toxicity and identity columns to booleans, in order to work with our neural net and metrics calculcations.  For this tutorial, we will consider any value >= 0.5 as True (i.e. a comment should be considered toxic if 50% or more crowd raters labeled it as toxic).  Note that this code also converts missing identity fields to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxicity</th>\n",
       "      <th>male</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>He got his money... now he lies in wait till a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Mad dog will surely put the liberals in mental...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>And Trump continues his lifelong cowardice by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Cry me a river, why don't you.\\nDrinking, drug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>That's right. They are not normal. And I am st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxicity   male                                       comment_text\n",
       "0     False  False  He got his money... now he lies in wait till a...\n",
       "1      True  False  Mad dog will surely put the liberals in mental...\n",
       "2      True  False  And Trump continues his lifelong cowardice by ...\n",
       "3     False  False  Cry me a river, why don't you.\\nDrinking, drug...\n",
       "4      True  False  That's right. They are not normal. And I am st..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all identities\n",
    "identity_columns = [\n",
    "    'male', 'female', 'transgender', 'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian',\n",
    "    'bisexual', 'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu', 'buddhist',\n",
    "    'atheist', 'other_religion', 'black', 'white', 'asian', 'latino', 'other_race_or_ethnicity',\n",
    "    'physical_disability', 'intellectual_or_learning_disability', 'psychiatric_or_mental_illness', 'other_disability']\n",
    "\n",
    "def convert_to_bool(df, col_name):\n",
    "    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n",
    "\n",
    "for df in [train_v1_df, validate_df, test_df]:\n",
    "    for col in ['toxicity'] + identity_columns:\n",
    "        convert_to_bool(df, col)\n",
    "    \n",
    "train_v1_df[['toxicity', 'male', 'comment_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train Models\n",
    "\n",
    "This code creates and trains a convolutional neural net using the Keras framework.  This neural net accepts a text comment, encoding as a sequence of integers, and outputs a probably that the comment is toxic.  Don't worry if you do not understand all of this code, as we will be treating this neural net as a black box later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 250\n",
    "MAX_NUM_WORDS = 10000\n",
    "TOXICITY_COLUMN = 'toxicity'\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "EMBEDDINGS_PATH = '../data/glove.6B/glove.6B.100d.txt'\n",
    "EMBEDDINGS_DIMENSION = 100\n",
    "DROPOUT_RATE = 0.3\n",
    "LEARNING_RATE = 0.00005\n",
    "NUM_EPOCHS = 1     # TODO: increase this\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_text(texts, tokenizer):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "def train_model(train_df, validate_df, tokenizer):\n",
    "    # Prepare data\n",
    "    train_text = pad_text(train_df[TEXT_COLUMN], tokenizer)\n",
    "    train_labels = to_categorical(train_df[TOXICITY_COLUMN])\n",
    "    validate_text = pad_text(validate_df[TEXT_COLUMN], tokenizer)\n",
    "    validate_labels = to_categorical(validate_df[TOXICITY_COLUMN])\n",
    "\n",
    "    # Load embeddings\n",
    "    embeddings_index = {}\n",
    "    with open(EMBEDDINGS_PATH) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1,\n",
    "                                 EMBEDDINGS_DIMENSION))\n",
    "    num_words_in_embedding = 0\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            num_words_in_embedding += 1\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # Create model layers.\n",
    "    def get_convolutional_neural_net_layers():\n",
    "        \"\"\"Returns (input_layer, output_layer)\"\"\"\n",
    "        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "        embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n",
    "                                    EMBEDDINGS_DIMENSION,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                    trainable=False)\n",
    "        x = embedding_layer(sequence_input)\n",
    "        x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(5, padding='same')(x)\n",
    "        x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(5, padding='same')(x)\n",
    "        x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
    "        x = MaxPooling1D(40, padding='same')(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dropout(DROPOUT_RATE)(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        preds = Dense(2, activation='softmax')(x)\n",
    "        return sequence_input, preds\n",
    "\n",
    "    # Compile model.\n",
    "    input_layer, output_layer = get_convolutional_neural_net_layers()\n",
    "    model = Model(input_layer, output_layer)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=RMSprop(lr=LEARNING_RATE),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    # Train model.\n",
    "    model.fit(train_text,\n",
    "              train_labels,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=NUM_EPOCHS,\n",
    "              validation_data=(validate_text, validate_labels),\n",
    "              verbose=2)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1285973 samples, validate on 161193 samples\n",
      "Epoch 1/1\n",
      "1241s - loss: 0.2033 - acc: 0.9299 - val_loss: 0.1736 - val_acc: 0.9381\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME_V1 = 'fat_star_tutorial_v1'\n",
    "tokenizer_v1 = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_v1.fit_on_texts(train_v1_df[TEXT_COLUMN])\n",
    "model_v1 = train_model(train_v1_df, validate_df, tokenizer_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score test set with the new model\n",
    "\n",
    "Using our new model, we can score the set of test comments for toxicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments_padded = pad_text(test_df[TEXT_COLUMN], tokenizer_v1)\n",
    "test_df[MODEL_NAME_V1] = model_v1.predict(test_comments_padded)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some records to compare our model resulsts with the correct labels\n",
    "test_df[[TOXICITY_COLUMN, TEXT_COLUMN, MODEL_NAME_V1]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure bias\n",
    "\n",
    "Using metrics based on Pinned AUC and the Mann Whitney U test, we can measure our model for biases against different identity groups.  We only calculate bias metrics on identities that are refered to in 100 or more comments, to minimize noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of identity columns that have >= 100 True records.  This will remove groups such\n",
    "# as \"other_disability\" which do not have enough records to calculate meaningful metrics.\n",
    "identities_with_over_100_records = []\n",
    "for identity in identity_columns:\n",
    "    num_records = len(test_df.query(identity + '==True'))\n",
    "    if num_records >= 100:\n",
    "        identities_with_over_100_records.append(identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalized_pinned_auc(df, subgroup, model_name):\n",
    "    subgroup_non_toxic = df[df[subgroup] & ~df[TOXICITY_COLUMN]]\n",
    "    subgroup_toxic = df[df[subgroup] & df[TOXICITY_COLUMN]]\n",
    "    background_non_toxic = df[~df[subgroup] & ~df[TOXICITY_COLUMN]]\n",
    "    background_toxic = df[~df[subgroup] & df[TOXICITY_COLUMN]]\n",
    "    \n",
    "    within_subgroup_mwu = normalized_mwu(subgroup_non_toxic, subgroup_toxic, model_name)\n",
    "    cross_negative_mwu = normalized_mwu(subgroup_non_toxic, background_toxic, model_name)\n",
    "    cross_positive_mwu = normalized_mwu(background_non_toxic, subgroup_toxic, model_name)\n",
    "    \n",
    "    return np.mean([1 - within_subgroup_mwu, 1 - cross_negative_mwu, 1 - cross_positive_mwu])\n",
    "\n",
    "def normalized_mwu(data1, data2, model_name):\n",
    "    \"\"\"Returns the number of pairs where the datapoint in data1 has a greater score than that from data2.\"\"\" \n",
    "    scores_1 = data1[model_name]\n",
    "    scores_2 = data2[model_name]\n",
    "    n1 = len(scores_1)\n",
    "    n2 = len(scores_2)\n",
    "    u, _ = stats.mannwhitneyu(scores_1, scores_2, alternative = 'less')\n",
    "    return u/(n1*n2)\n",
    "\n",
    "def compute_pinned_auc(df, identity, model_name):\n",
    "    # Create combined_df, containing an equal number of comments that refer to the identity, and\n",
    "    # that belong to the background distribution.\n",
    "    identity_df = df[df[identity]]\n",
    "    nonidentity_df = df[~df[identity]].sample(len(identity_df), random_state=25)\n",
    "    combined_df = pd.concat([identity_df, nonidentity_df])\n",
    "\n",
    "    # Calculate the Pinned AUC\n",
    "    true_labels = combined_df[TOXICITY_COLUMN]\n",
    "    predicted_labels = combined_df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "def get_bias_metrics(df, model_name):\n",
    "    bias_metrics_df = pd.DataFrame({\n",
    "        'subgroup': identities_with_over_100_records,\n",
    "        'pinned_auc': [compute_pinned_auc(df, identity, model_name)\n",
    "                       for identity in identities_with_over_100_records],\n",
    "        'normalized_pinned_auc': [compute_normalized_pinned_auc(df, identity, model_name)\n",
    "                                  for identity in identities_with_over_100_records]\n",
    "    })\n",
    "    # Re-order columns and sort bias metrics\n",
    "    return bias_metrics_df[['subgroup', 'pinned_auc', 'normalized_pinned_auc']].sort_values('pinned_auc')\n",
    "\n",
    "def calculate_overall_auc(df, model_name):\n",
    "    true_labels = df[TOXICITY_COLUMN]\n",
    "    predicted_labels = df[model_name]\n",
    "    return metrics.roc_auc_score(true_labels, predicted_labels)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_metrics_df = get_bias_metrics(test_df, MODEL_NAME_V1)\n",
    "bias_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_overall_auc(test_df, MODEL_NAME_V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can graph a histogram of comment scores in each identity.  In the following graphs, the X axis represents the toxicity score given by our new model, and the Y axis represents the comment count.  Blue values are comment whose true label is non-toxic, while red values are those whose true label is toxic.\n",
    "\n",
    "We can see that for some identities such as Asian, the model scores most non-toxic comments as less than 0.2 and most toxic comments as greater than 0.2.  This indicates that for the Asian identity, our model is able to distinguish between toxic and non-toxic comments.  However, for the black identity, there are many non-toxic comments with scores over 0.5, along with many toxic comments with scores of less than 0.5.  This shows that for the black identity, our model will be less accurate at separating toxic comments from non-toxic comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot toxicity distributions of different identities to visualize bias.\n",
    "def plot_histogram(identity):\n",
    "    toxic_scores = test_df.query(identity + ' == True & toxicity == True')[MODEL_NAME_V1]\n",
    "    non_toxic_scores = test_df.query(identity + ' == True & toxicity == False')[MODEL_NAME_V1]\n",
    "    sns.distplot(non_toxic_scores, color=\"skyblue\", axlabel=identity)\n",
    "    sns.distplot(toxic_scores, color=\"red\", axlabel=identity)\n",
    "    plt.figure()\n",
    "\n",
    "for identity in bias_metrics_df['subgroup']:\n",
    "    plot_histogram(identity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain model to reduce bias\n",
    "\n",
    "One possible reason for bias in the model may be that our training data is baised.  In our case, our initial training data contained a higher percentage of toxic vs non-toxic comments for the \"homosexual_gay_or_lesbian\" identity.  We have another dataset which contains additional non-toxic comments that refer to the \"homosexual_gay_or_lesbian\" group.  If we train a new model using this data, we should make a small improvement in bias against this category (TODO: verify this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new training data and convert fields to booleans.\n",
    "train_v2_df = pd.read_csv('../input/fat-star-tutorial-data/public_train_v2.csv')\n",
    "for col in ['toxicity'] + identity_columns:\n",
    "    convert_to_bool(train_v2_df, col)\n",
    "\n",
    "# Create a new model using the same structure as our model_v1.\n",
    "MODEL_NAME_V2 = 'fat_star_tutorial_v2'\n",
    "tokenizer_v2 = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_v2.fit_on_texts(train_v2_df[TEXT_COLUMN])\n",
    "model_v2 = train_model(train_v2_df, validate_df, tokenizer_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments_padded_v2 = pad_text(test_df[TEXT_COLUMN], tokenizer_v2)\n",
    "test_df[MODEL_NAME_V2] = model_v2.predict(test_comments_padded_v2)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_metrics_v2_df = get_bias_metrics(test_df, MODEL_NAME_V2)\n",
    "bias_metrics_v2_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
